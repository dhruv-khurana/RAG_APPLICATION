{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI API Demo"
      ],
      "metadata": {
        "id": "JzMYbSKGnQAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install OpenAI & PYPDF2 Libraries\n",
        "\n",
        "%pip install openai==1.55.3 httpx==0.27.2 --force-reinstall --quiet\n",
        "\n",
        "%pip install PyPDF2"
      ],
      "metadata": {
        "id": "A95VMciWnzTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrival Augmented Generation Application (RAG) Demo (By Dhruv)\n",
        "**What We Will Do:**  We will create a RAG Application that can be used as a math tutor to help walk you through solutions to the UKMT Intermediate Maths Challenge\n",
        "\n",
        "**The Data That We Will Use:**\n",
        "\n",
        "https://ukmt.org.uk/wp-content/uploads/2024/02/IMC_2024-Paper.pdf\n",
        "\n",
        "⬆ A PDF of Maths Competition Past Papers ⬆"
      ],
      "metadata": {
        "id": "6kDZVIKaJ3Yd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS2ypZWyAUne"
      },
      "source": [
        "**What is Retrival Augmented Generation?**\n",
        "\n",
        "Standalone LLMs (including ChatGPT) often encounter limitations when tasked with specific queries. These limitations stem from their inability to access external information or context beyond their pre-trained knowledge.\n",
        "\n",
        "To overcome this challenge, researchers have developed innovative approaches like Retrieval Augmented Generation (RAG). Motivated by the need to enhance language models with access to external knowledge sources, RAG combines the power of traditional language models with retrieval mechanisms, enabling them to generate more informed and contextually relevant responses."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install OpenAI & PYPDF2 Libraries\n",
        "\n",
        "%pip install openai==1.55.3 httpx==0.27.2 --force-reinstall --quiet\n",
        "\n",
        "%pip install PyPDF2"
      ],
      "metadata": {
        "id": "zKxFdXxjChwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  Download Our Data\n",
        "\n",
        "# you can put in a link to another PDF as well, but remember to also change the pdf_name variable when you do that and the system prompt\n",
        "\n",
        "!wget https://ukmt.org.uk/wp-content/uploads/2024/02/IMC_2024-Paper.pdf\n",
        "\n",
        "pdf_name = 'IMC_2024-Paper.pdf'"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Mb9fVk6OzoWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1: Import Libraries\n",
        "\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "I7Myn3_iDErR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2: Read PDF File\n",
        "\n",
        "pdf = PdfReader(pdf_name)\n",
        "\n",
        "\n",
        "text = \"\"\n",
        "\n",
        "\n",
        "for page_num in range(len(pdf.pages)):\n",
        "\n",
        "    page = pdf.pages[page_num]\n",
        "\n",
        "    text += page.extract_text()\n",
        "\n",
        "    text += ' '"
      ],
      "metadata": {
        "id": "RJTmAav0DWxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 3: Create ask_question Function\n",
        "\n",
        "\n",
        "messages = []\n",
        "\n",
        "\n",
        "def ask_question(question):\n",
        "\n",
        "\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "# you can replace this OpenAI API key with your own\n",
        "\n",
        "    api_key = ''\n",
        "\n",
        "\n",
        "    client = OpenAI(api_key=api_key)\n",
        "\n",
        "\n",
        "    response = client.chat.completions.create(model = 'chatgpt-4o-latest', messages = messages, temperature=0.1, stream=True)\n",
        "\n",
        "\n",
        "# streaming (typing) effect in the output\n",
        "\n",
        "    out = ''\n",
        "\n",
        "    div = 0\n",
        "\n",
        "    for chunk in response:\n",
        "\n",
        "          if chunk.choices[0].delta.content is not None:\n",
        "\n",
        "              content_delta = chunk.choices[0].delta.content\n",
        "\n",
        "              content_delta = content_delta.replace('*', '')\n",
        "\n",
        "              content_delta = content_delta.replace('#', '')\n",
        "\n",
        "              content_delta = content_delta.replace(\"\\\\\", '')\n",
        "\n",
        "              content_delta = content_delta.replace(\"frac\", '')\n",
        "\n",
        "              content_delta = content_delta.replace(\"{\", '')\n",
        "\n",
        "              if '}' in content_delta and div == 0:\n",
        "                content_delta = content_delta.replace(\"}\", '/')\n",
        "                div += 1\n",
        "              else:\n",
        "                content_delta = content_delta.replace(\"}\", '')\n",
        "                div -= 0\n",
        "\n",
        "              print(content_delta, end=\"\")\n",
        "\n",
        "              out += content_delta\n",
        "\n",
        "              if len(content_delta) - 1   == ('.' or ',' or '!'):\n",
        "\n",
        "                print('\\n')\n",
        "\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "sj7JrZ91jgb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 4: Run Program\n",
        "\n",
        "while True:\n",
        "\n",
        "\n",
        "      question = input(\"\\n  User:  \")\n",
        "\n",
        "\n",
        "      if question.lower() == 'exit':\n",
        "\n",
        "          messages.clear()\n",
        "\n",
        "          break\n",
        "\n",
        "\n",
        "      messages.append({'role':'system',\n",
        "             'content':f'you are helping students understand {question} problems in this past paper: {text}, As an AI assistant, your role is to help students understand and solve math problems from their past papers. Instead of providing the answer outright, engage the student in a conversation for each step.'})\n",
        "\n",
        "\n",
        "      answer = ask_question(question)\n",
        "\n",
        "\n",
        "      messages.append({'role':'assistant', 'content':answer})"
      ],
      "metadata": {
        "id": "S4oWAh-HkKR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "We have just implemented a very basic Retrieval Augmented Generation (RAG) system by providing the PDF text and the user's question as a single prompt to the language model. This approach works well for situations where the relevant context can fit within the model's context window. However, more complex RAG architectures are required when dealing with larger knowledge bases or multiple documents that cannot be concatenated and fit within the context window.\n",
        "\n",
        "These advanced RAG systems typically have a separate retrieval component that efficiently searches through the knowledge base and retrieves the most relevant passages or documents based on the user's query. The retrieved context is then provided to the language model along with the query, allowing the model to generate a response based on the relevant information. Perplexity, for example, utilizes its own retrieval algorithm to find the most relevant context from its knowledge base, enabling it to provide accurate and contextually relevant responses even for complex queries spanning multiple documents or domains."
      ],
      "metadata": {
        "id": "8LA3Zup9IMmA"
      }
    }
  ]
}
